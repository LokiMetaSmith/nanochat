# Running nanochat with Docker for ROCm

This guide provides instructions on how to build and run the nanochat project using Docker, which is the recommended method for ensuring a correct and reproducible environment, especially for AMD ROCm GPUs.

## Prerequisites

- [Docker](https://docs.docker.com/engine/install/) installed on your system.
- An AMD GPU with ROCm drivers installed on the host machine.

## Step 1: Build the Docker Image

First, you need to build the Docker image. This command will create a Docker image named `nanochat-rocm` with all the necessary dependencies and the correct version of PyTorch for ROCm.

Open a terminal in the root of the nanochat project and run the following command:

```bash
sudo docker build -t nanochat-rocm .
```

If you encounter an error that the base image is not found, you may need to find a different tag for the `rocm/pytorch` image on [Docker Hub](https://hub.docker.com/r/rocm/pytorch/tags) and update the `FROM` line in the `Dockerfile`.

## Step 2: Run the Docker Container

Once the image is built, you can run the `speedrun.sh` script inside a Docker container. The following command starts a container, mounts the project directory, and grants the container access to your GPU.

```bash
sudo docker run --rm -it --device=/dev/kfd --device=/dev/dri --security-opt seccomp=unconfined -v $(pwd):/home/user/app nanochat-rocm
```

### Explanation of Docker Flags:

- `--rm`: Automatically removes the container when it exits.
- `-it`: Runs the container in interactive mode with a pseudo-TTY.
- `--device=/dev/kfd --device=/dev/dri`: Grants the container access to the GPU devices.
- `--security-opt seccomp=unconfined`: Disables the default seccomp profile, which can interfere with GPU access.
- `-v $(pwd):/home/user/app`: Mounts the current directory (your nanochat project) into the container's working directory. This allows you to see the results and reports generated by the script on your host machine.

## Step 3: The `speedrun.sh` Script

The `speedrun.sh` script will automatically start executing inside the container. It will perform all the necessary steps, from downloading the dataset and training the tokenizer to pre-training and fine-tuning the model.

You can monitor the progress in your terminal. When the script is finished, the container will exit, and you will find the results and reports in your project directory.