# General Configuration
# Base directory for nanochat outputs and cache (default: ~/.cache/nanochat)
NANOCHAT_BASE_DIR=~/.cache/nanochat
# Set to 1 to skip stability workarounds (e.g. for Strix Halo)
NANOCHAT_SKIP_WORKAROUNDS=0

# Serving Configuration (scripts/serve.py)
NANOCHAT_PORT=8000
NANOCHAT_HOST=0.0.0.0
# Number of GPUs to use for serving
NANOCHAT_NUM_GPUS=1
# Model source: 'sft', 'base', etc.
NANOCHAT_SOURCE=sft
# Specific model tag to load (e.g. 'd12')
NANOCHAT_MODEL_TAG=
# Specific checkpoint step to load
NANOCHAT_STEP=
# Device type override (e.g. 'cuda', 'cpu')
NANOCHAT_DEVICE_TYPE=
# Data type (bfloat16, float16, float32)
NANOCHAT_DTYPE=bfloat16
# Enable mock mode for testing API without loading model (1=True, 0=False)
NANOCHAT_MOCK=0

# Authentication
# Hugging Face token for downloading datasets/models
HF_TOKEN=

# Training & Logging
# Weights & Biases run name
WANDB_RUN=
# Number of OpenMP threads (usually set automatically)
# OMP_NUM_THREADS=1

# Advanced / Hardware Overrides
# PyTorch memory allocator configuration
# PYTORCH_ALLOC_CONF=expandable_segments:True
# PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
# PYTORCH_HIP_ALLOC_CONF=expandable_segments:True

# ROCm / AMD Specific
# Enable experimental Triton features for ROCm
# TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1
# Prefer hipBLASLt (0 or 1)
# TORCH_BLAS_PREFER_HIPBLASLT=0
# Path to LLD linker for Triton on ROCm
# TRITON_HIP_LLD_PATH=/opt/rocm/llvm/bin/ld.lld
# Override GFX version (e.g. for Strix Halo)
# HSA_OVERRIDE_GFX_VERSION=11.5.1
