## Tokenizer training
timestamp: 2025-12-11 22:21:41

- max_chars: 10,000,000,000
- doc_cap: 10,000
- vocab_size: 1024
- train_time: 0.0608
- num_special_tokens: 9
- token_bytes_min: 1
- token_bytes_max: 7
- token_bytes_mean: 2.0808
- token_bytes_std: 0.9953
